{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b10b11-7087-4793-850b-0c45937c088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389b1dd-1d7c-4eca-a5a0-0108672c7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ BEGIN NUMPY STARTER CODE #################################################\n",
    "def sigmoid(x):\n",
    "    #Numerically stable sigmoid function.\n",
    "    #Taken from: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05b9159-61f9-4055-ad03-bfcfb87acf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logistic_distribution(x,a):\n",
    "    #np.random.seed(1)\n",
    "    num_samples = len(x)\n",
    "    y = np.empty(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        y[i] = np.random.binomial(1,logistic_positive_prob(x[i],a))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5342663-e70c-4fcb-8558-c62465274c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    x= create_input_values(2,100)\n",
    "    a=np.array([12,12])\n",
    "    y=sample_logistic_distribution(x,a)\n",
    "\n",
    "    return x,y\n",
    "    \n",
    "################################ END NUMPY STARTER CODE ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5100fc4-f640-4c15-a482-7048882645df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################ BEGIN PYTORCH STARTER CODE ################################################\n",
    "\n",
    "class TorchLogisticClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, num_features):\n",
    "    super().__init__()\n",
    "    self.weights = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "  def forward(self, x_vector):\n",
    "    logit = torch.dot(self.weights, x_vector)\n",
    "    prob = torch.sigmoid(logit)\n",
    "    return prob\n",
    "\n",
    "\n",
    "def loss_fn(y_predicted, y_observed):\n",
    "    return -1 * (y_observed * torch.log(y_predicted)\n",
    "                 + (1 - y_observed) * torch.log(1 - y_predicted))\n",
    "\n",
    "def extract_num_features(dataset):\n",
    "    first_example = dataset[0]\n",
    "    # first_example is a pair (x,y), where x is a vector of features and y is 0 or 1\n",
    "    # note that both x and y are torch tensors\n",
    "    first_example_x = first_example[0]\n",
    "    first_example_y = first_example[1]\n",
    "    num_features = first_example_x.size(0)\n",
    "    return num_features\n",
    "\n",
    "def nonbatched_gradient_descent(dataset, num_epochs=10, learning_rate=0.01):\n",
    "    num_features = extract_num_features(dataset)\n",
    "    model = TorchLogisticClassifier(num_features)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for i in range(num_epochs):\n",
    "        for d_x, d_y in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(d_x)\n",
    "            loss = loss_fn(prediction, d_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def generate_nonbatched_data(num_features=3, num_examples=100):\n",
    "    x_vectors = [torch.randn(num_features) for _ in range(num_examples)]\n",
    "    prob_val = 0.5 * torch.ones(1)\n",
    "    y_vectors = [torch.bernoulli(prob_val) for _ in range(num_examples)]\n",
    "\n",
    "    dataset = list(zip(x_vectors, y_vectors))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    nonbatched_dataset = generate_nonbatched_data()\n",
    "    nonbatched_gradient_descent(nonbatched_dataset)\n",
    "    \n",
    "################################ END PYTORCH STARTER CODE ###################################################\n",
    "\n",
    "\n",
    "# NOTICE: DO NOT EDIT FUNCTION SIGNATURES \n",
    "# PLEASE FILL IN FREE RESPONSE AND CODE IN THE PROVIDED SPACES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8d8a11-80a4-4c31-84ac-d3de4350eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 1\n",
    "def logistic_positive_prob(x,a):\n",
    "    ax=np.dot(a,x)\n",
    "    return sigmoid(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c8955-4a0d-4631-a32b-48a7897eb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2\n",
    "def logistic_derivative_per_datapoint(y_i,x_i,a,j):\n",
    "    result=-(y_i-logistic_positive_prob(x_i,a))*x_i[j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b930ce-8c69-4c0b-b694-7f826f2597ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 3\n",
    "def logistic_partial_derivative(y,x,a,j):\n",
    "    partial_des=[]\n",
    "    for i in j:\n",
    "        partial_de=logistic_derivative_per_datapoint(y[i],x[i],a,j)\n",
    "        partial_des.append(partial_de)\n",
    "    result=np.mean(partial_des)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2c49f-5a49-4b0d-89e7-e5b46f24ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 4\n",
    "def compute_logistic_gradient(a,y,x):\n",
    "    k=len(a)\n",
    "    gradients=np.zeros(k)\n",
    "    for j in range(k):\n",
    "        gradients[j]=logistic_partial_derivative(y,x,a,j)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b87fcd-bb09-46ed-ac00-15c5f096898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 5\n",
    "def gradient_update(a,lr,gradient):\n",
    "    a=a-gradient*lr\n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca501c2-5eb1-4012-b338-5e27ebb9cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 6\n",
    "def gradient_descent_logistic(initial_a,lr,num_iterations,y,x):\n",
    "    for i in range(num_iterations):\n",
    "        gradient=compute_logistic_gradient(initial_a,y,x)\n",
    "        initial_a=gradient_update(initial_a,lr,gradient)\n",
    "    return initial_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f3731-b173-48ba-a1c9-fa0bf11fd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 7\n",
    "#This function _init_ creates a learnable parameter (weights) initialized to zeros, with a size of num_features\n",
    "#The __init__ function is called when we create an instance of the TorchLogisticClassifier class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d7a91-ef50-4c87-ad92-d46853f223a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 8\n",
    "#The forward method first computes the logit and then applies the sigmoid function to the logit to transform it into a probability\n",
    "#The line of code: prediction = model(d_x) computes predictions for a specific input vector d_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086c3f7-7701-426b-9933-0366d9b5cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 9\n",
    "def batched_gradient_descent(dataset, num_epochs=10, learning_rate=0.01, batch_size=2):\n",
    "    num_features=extract_num_features(dataset)\n",
    "    model=TorchLogisticClassifier(num_features)\n",
    "    optimizer=optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_start in range(0,len(dataset),batch_size):\n",
    "            batch=dataset[batch_start:batch_start+batch_size]\n",
    "            batch_x=torch.stack([d_x for d_x,d_y in batch])\n",
    "            batch_y=torch.stack([d_y for d_x,d_y in batch])\n",
    "            optimizer.zero_grad()\n",
    "            predictions=torch.stack([model(d_x) for d_x in batch_x])\n",
    "            loss=torch.mean(torch.stack([loss_fn(pred,true_y) for pred,true_y in zip(predictions, batch_y)]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f6670-e90f-4ac0-8811-8e124abcf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEMS 10-12\n",
    "def split_into_batches(dataset, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # Append a batch consisting of the current batch_size number of elements\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "    \n",
    "def alt_gradient_descent(dataset, num_epochs=10, learning_rate=0.01, batch_size=2):\n",
    "    num_features = extract_num_features(dataset)\n",
    "    model = TorchLogisticClassifier(num_features)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batches = split_into_batches(dataset, batch_size)\n",
    "    for i in range(num_epochs):\n",
    "        # optimizer.zero_grad() # 1\n",
    "        for batch in batches:\n",
    "            # optimizer.zero_grad() # 2\n",
    "            for d_x, d_y in batch:\n",
    "                # optimizer.zero_grad() # 3\n",
    "                prediction = model(d_x)\n",
    "                loss = loss_fn(prediction, d_y)\n",
    "                loss.backward()\n",
    "                # optimizer.step() # C\n",
    "            # optimizer.step() # B\n",
    "        # optimizer.step() # A\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ed6d1-5e5d-48b6-b4b2-85da05e25b7a",
   "metadata": {},
   "source": [
    "# PROBLEM 10\n",
    "$$\n",
    "\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\nabla L(\\mathbf{w}(t) \\mid B_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd58c1-1262-40c0-a091-2226c505767e",
   "metadata": {},
   "source": [
    "# PROBLEM 11\n",
    "$$\n",
    "\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\cdot \\sum_{i=1}^{t} \\sum_{j=1}^{m} \\nabla L(\\mathbf{w}(i) \\mid B_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6e4a5-9893-4f2b-b234-06a88a5aaaac",
   "metadata": {},
   "source": [
    "# PROBLEM 12\n",
    "$$\n",
    "\\text{alt\\_gradient\\_descent\\_3B} \\quad \\text{with arguments:} \\quad (dataset, \\, num\\_epochs, \\, learning\\_rate = lr, \\, batch\\_size = k)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
